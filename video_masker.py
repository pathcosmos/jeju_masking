#!/usr/bin/env python3
"""
ë¹„ë””ì˜¤ ë§ˆìŠ¤í‚¹ í´ë˜ìŠ¤ ëª¨ë“ˆ
- VideoMasker: ê¸°ë³¸ ë§ˆìŠ¤í‚¹ í´ë˜ìŠ¤
- VideoMaskerOptimized: ìµœì í™”ëœ ë§ˆìŠ¤í‚¹ í´ë˜ìŠ¤ (GPU ê°€ì†, ë©€í‹°ìŠ¤ë ˆë”©, íŠ¸ë˜í‚¹ ë³´ê°„)
"""

import os
import time
import json
import tempfile
import subprocess
from pathlib import Path
from datetime import datetime
from collections import deque
from threading import Thread, Event
from queue import Queue, Empty

import cv2
import yaml
import numpy as np
from ultralytics import YOLO

from masking_utils import (
    setup_logger, apply_blur, apply_mosaic,
    expand_box, get_plate_region, get_all_plate_regions,
    validate_box, validate_and_clip_box
)
from encoding_utils import (
    get_system_info, get_optimal_settings, build_nvenc_command, print_system_info
)
from high_performance import process_video_high_performance
from two_pass import analyze_video, encode_with_masks, process_video_2pass


# ============================================================
# ë””ë°”ì´ìŠ¤ ê°ì§€
# ============================================================

def get_optimal_device():
    """ìµœì  ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
    try:
        import torch
        if torch.cuda.is_available():
            return 'cuda'
        elif torch.backends.mps.is_available() and platform.processor() == 'arm':
            return 'mps'
    except Exception:
        pass
    return 'cpu'


def setup_cuda_optimization(device='cuda', gpu_id=0):
    """CUDA ìµœì í™” ì„¤ì •"""
    try:
        import torch
        if not torch.cuda.is_available():
            return False
        
        if gpu_id >= 0 and gpu_id < torch.cuda.device_count():
            torch.cuda.set_device(gpu_id)
        
        torch.backends.cudnn.enabled = True
        torch.backends.cudnn.benchmark = True
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        
        if hasattr(torch.cuda, 'memory'):
            torch.cuda.empty_cache()
        
        return True
    except Exception:
        return False


def create_custom_tracker_config(tracker_type, track_buffer, match_thresh):
    """
    ì»¤ìŠ¤í…€ íŠ¸ë˜ì»¤ ì„¤ì • íŒŒì¼ ìƒì„±
    - track_high_thresh: ë†’ì€ ì‹ ë¢°ë„ ì„ê³„ê°’ (í™•ì‹¤í•œ ê°ì§€)
    - track_low_thresh: ë‚®ì€ ì‹ ë¢°ë„ ì„ê³„ê°’ (ê¸°ì¡´ íŠ¸ë™ ìœ ì§€ìš©)
    - new_track_thresh: ìƒˆ íŠ¸ë™ ìƒì„± ì„ê³„ê°’
    - track_buffer: ê°ì§€ ëˆ„ë½ ì‹œ íŠ¸ë™ ìœ ì§€ í”„ë ˆì„ ìˆ˜
    """
    if tracker_type == "bytetrack":
        config = {
            'tracker_type': 'bytetrack',
            'track_high_thresh': 0.4,   # ë‚®ì¶¤: ë” ë§ì€ ê°ì§€ í¬í•¨
            'track_low_thresh': 0.05,   # ë‚®ì¶¤: ê¸°ì¡´ íŠ¸ë™ ë” ì˜¤ë˜ ìœ ì§€
            'new_track_thresh': 0.5,    # ë‚®ì¶¤: ìƒˆ íŠ¸ë™ ë” ì‰½ê²Œ ìƒì„±
            'track_buffer': track_buffer,
            'match_thresh': match_thresh,
            'fuse_score': True
        }
    else:
        config = {
            'tracker_type': 'botsort',
            'track_high_thresh': 0.4,
            'track_low_thresh': 0.05,
            'new_track_thresh': 0.5,
            'track_buffer': track_buffer,
            'match_thresh': match_thresh,
            'proximity_thresh': 0.5,
            'appearance_thresh': 0.25,
            'with_reid': False,
            'gmc_method': 'sparseOptFlow',
            'fuse_score': True
        }

    temp_file = tempfile.NamedTemporaryFile(
        mode='w', suffix='.yaml', delete=False, prefix=f'{tracker_type}_custom_'
    )
    yaml.dump(config, temp_file, default_flow_style=False)
    temp_file.close()
    return temp_file.name


# ============================================================
# ë©€í‹°ìŠ¤ë ˆë”© ì›Œì»¤
# ============================================================

class FFmpegPipeline:
    """FFmpeg í•˜ë“œì›¨ì–´ ê°€ì† íŒŒì´í”„ë¼ì¸ (NVDEC ë””ì½”ë”© + NVENC ì¸ì½”ë”©)"""
    
    def __init__(self, input_path, output_path, width, height, fps, 
                 use_hwaccel=True, use_hevc=False, start_time=None, end_time=None):
        self.input_path = input_path
        self.output_path = output_path
        self.width = width
        self.height = height
        self.fps = fps
        self.use_hwaccel = use_hwaccel
        self.use_hevc = use_hevc
        self.frame_size = width * height * 3
        
        self.decoder = None
        self.encoder = None
        
        # ë””ì½”ë” ëª…ë ¹ì–´ êµ¬ì„±
        decode_cmd = ['ffmpeg', '-hide_banner', '-loglevel', 'warning']
        
        # í•˜ë“œì›¨ì–´ ê°€ì† ë””ì½”ë”© (NVDEC) - cuvid ë””ì½”ë” ì‚¬ìš©
        if use_hwaccel:
            # HEVC/H.264 cuvid ë””ì½”ë”ë¡œ ì§ì ‘ ë””ì½”ë”©
            decode_cmd.extend(['-hwaccel', 'cuda', '-c:v', 'hevc_cuvid'])
        
        # ì‹œì‘/ì¢…ë£Œ ì‹œê°„ (ì…ë ¥ ì „ì— -ssë¡œ ë¹ ë¥¸ ì‹œí¬)
        if start_time:
            decode_cmd.extend(['-ss', start_time])
        decode_cmd.extend(['-i', input_path])
        if end_time:
            decode_cmd.extend(['-t', end_time])  # -to ëŒ€ì‹  -t (duration)
        
        # ì¶œë ¥ í˜•ì‹ (raw video) - GPUì—ì„œ CPUë¡œ ì „ì†¡ í›„ bgr24 ë³€í™˜
        if use_hwaccel:
            # scale_cudaë¡œ GPUì—ì„œ í¬ë§· ë³€í™˜ í›„ ë‹¤ìš´ë¡œë“œ
            decode_cmd.extend(['-vf', 'scale_cuda=format=bgr24,hwdownload,format=bgr24'])
        else:
            decode_cmd.extend(['-pix_fmt', 'bgr24'])
        
        decode_cmd.extend(['-f', 'rawvideo', '-'])
        
        # ì¸ì½”ë” ëª…ë ¹ì–´ êµ¬ì„±
        encode_cmd = ['ffmpeg', '-hide_banner', '-loglevel', 'error', '-y']
        encode_cmd.extend([
            '-f', 'rawvideo', '-pix_fmt', 'bgr24',
            '-s', f'{width}x{height}', '-r', str(fps),
            '-i', '-'
        ])
        
        if use_hevc:
            if use_hwaccel:
                # NVENC HEVC
                encode_cmd.extend([
                    '-c:v', 'hevc_nvenc', '-preset', 'p4', '-tune', 'hq',
                    '-rc', 'vbr', '-cq', '23', '-b:v', '0',
                    '-tag:v', 'hvc1'
                ])
            else:
                encode_cmd.extend(['-c:v', 'libx265', '-preset', 'medium', '-crf', '23'])
        else:
            if use_hwaccel:
                # NVENC H.264
                encode_cmd.extend([
                    '-c:v', 'h264_nvenc', '-preset', 'p4', '-tune', 'hq',
                    '-rc', 'vbr', '-cq', '23', '-b:v', '0'
                ])
            else:
                encode_cmd.extend(['-c:v', 'libx264', '-preset', 'medium', '-crf', '23'])
        
        encode_cmd.extend(['-an', output_path])
        
        self.decode_cmd = decode_cmd
        self.encode_cmd = encode_cmd
    
    def start(self):
        """íŒŒì´í”„ë¼ì¸ ì‹œì‘"""
        import subprocess
        self.decoder = subprocess.Popen(
            self.decode_cmd, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.PIPE,
            bufsize=self.frame_size * 10  # 10 í”„ë ˆì„ ë²„í¼
        )
        self.encoder = subprocess.Popen(
            self.encode_cmd,
            stdin=subprocess.PIPE,
            stderr=subprocess.PIPE,
            bufsize=self.frame_size * 10
        )
        return self
    
    def read_frame(self):
        """í”„ë ˆì„ ì½ê¸°"""
        if self.decoder is None:
            return None
        
        raw_frame = self.decoder.stdout.read(self.frame_size)
        if len(raw_frame) != self.frame_size:
            return None
        
        import numpy as np
        frame = np.frombuffer(raw_frame, dtype=np.uint8).reshape((self.height, self.width, 3))
        return frame.copy()  # ì“°ê¸° ê°€ëŠ¥í•œ ë³µì‚¬ë³¸
    
    def write_frame(self, frame):
        """í”„ë ˆì„ ì“°ê¸°"""
        if self.encoder is not None:
            self.encoder.stdin.write(frame.tobytes())
    
    def close(self):
        """íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ"""
        if self.decoder:
            self.decoder.stdout.close()
            self.decoder.wait()
        if self.encoder:
            self.encoder.stdin.close()
            self.encoder.wait()
    
    def __enter__(self):
        return self.start()
    
    def __exit__(self, *args):
        self.close()


class NVDECDecoder:
    """
    FFmpeg NVDEC í•˜ë“œì›¨ì–´ ê°€ì† ë””ì½”ë” (GPU ë””ì½”ë”©)
    - cv2.VideoCapture ëŒ€ë¹„ 2-3ë°° ë¹ ë¥¸ ë””ì½”ë”©
    - GPUì—ì„œ ì§ì ‘ ë””ì½”ë”©í•˜ì—¬ CPU ë¶€í•˜ ìµœì†Œí™”
    """

    # ì½”ë±ë³„ cuvid ë””ì½”ë” ë§¤í•‘
    CUVID_DECODERS = {
        'hevc': 'hevc_cuvid',
        'h264': 'h264_cuvid',
        'h265': 'hevc_cuvid',
        'av1': 'av1_cuvid',
        'vp9': 'vp9_cuvid',
        'vp8': 'vp8_cuvid',
        'mpeg4': 'mpeg4_cuvid',
        'mpeg2video': 'mpeg2_cuvid',
        'mpeg1video': 'mpeg1_cuvid',
        'mjpeg': 'mjpeg_cuvid',
        'vc1': 'vc1_cuvid',
    }

    def __init__(self, input_path, width, height, start_time=None, end_time=None):
        self.input_path = input_path
        self.width = width
        self.height = height
        self.frame_size = width * height * 3
        self.decoder = None

        # ì…ë ¥ ë¹„ë””ì˜¤ ì½”ë± ê°ì§€
        codec = self._detect_codec(input_path)
        cuvid_decoder = self.CUVID_DECODERS.get(codec)

        # ë””ì½”ë” ëª…ë ¹ì–´ êµ¬ì„±
        decode_cmd = ['ffmpeg', '-hide_banner', '-loglevel', 'error']

        # NVDEC í•˜ë“œì›¨ì–´ ê°€ì† (cuvid ë””ì½”ë” ëª…ì‹œì  ì§€ì •)
        if cuvid_decoder:
            decode_cmd.extend(['-hwaccel', 'cuda', '-c:v', cuvid_decoder])
        else:
            # ì§€ì›ë˜ì§€ ì•ŠëŠ” ì½”ë±ì€ ì†Œí”„íŠ¸ì›¨ì–´ ë””ì½”ë”©
            decode_cmd.extend(['-hwaccel', 'cuda'])

        # ì‹œì‘ ì‹œê°„ (ì…ë ¥ ì „ì— -ssë¡œ ë¹ ë¥¸ ì‹œí¬)
        if start_time:
            decode_cmd.extend(['-ss', str(start_time)])

        decode_cmd.extend(['-i', input_path])

        # ì¢…ë£Œ ì‹œê°„
        if end_time:
            if start_time:
                duration = end_time - start_time
                decode_cmd.extend(['-t', str(duration)])
            else:
                decode_cmd.extend(['-t', str(end_time)])

        # ì¶œë ¥ í¬ë§· (FFmpegì´ ìë™ìœ¼ë¡œ GPU->CPU ì „ì†¡)
        decode_cmd.extend([
            '-f', 'rawvideo',
            '-pix_fmt', 'bgr24',
            '-'
        ])

        self.decode_cmd = decode_cmd
        self._cuvid_decoder = cuvid_decoder

    def _detect_codec(self, input_path):
        """ì…ë ¥ ë¹„ë””ì˜¤ì˜ ì½”ë± ê°ì§€"""
        try:
            result = subprocess.run(
                ['ffprobe', '-v', 'error', '-select_streams', 'v:0',
                 '-show_entries', 'stream=codec_name',
                 '-of', 'default=noprint_wrappers=1:nokey=1', input_path],
                capture_output=True, text=True, timeout=10
            )
            return result.stdout.strip().lower()
        except Exception:
            return None

    def start(self):
        """ë””ì½”ë” ì‹œì‘"""
        self.decoder = subprocess.Popen(
            self.decode_cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            bufsize=self.frame_size * 32  # í° ë²„í¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
        )
        return self

    def read_frame(self):
        """í”„ë ˆì„ ì½ê¸°"""
        if self.decoder is None:
            return None

        raw_frame = self.decoder.stdout.read(self.frame_size)
        if len(raw_frame) != self.frame_size:
            return None

        frame = np.frombuffer(raw_frame, dtype=np.uint8).reshape((self.height, self.width, 3))
        return frame.copy()

    def close(self):
        """ë””ì½”ë” ì¢…ë£Œ"""
        if self.decoder:
            self.decoder.stdout.close()
            self.decoder.terminate()
            self.decoder.wait()
            self.decoder = None

    def __enter__(self):
        return self.start()

    def __exit__(self, *args):
        self.close()


class FrameReader(Thread):
    """ë¹„ë™ê¸° í”„ë ˆì„ ì½ê¸° ìŠ¤ë ˆë“œ"""

    def __init__(self, cap, queue, start_frame, end_frame):
        super().__init__(daemon=True)
        self.cap = cap
        self.queue = queue
        self.start_frame = start_frame
        self.end_frame = end_frame
        self.stopped = Event()

    def run(self):
        frame_idx = self.start_frame
        while not self.stopped.is_set() and frame_idx < self.end_frame:
            if self.queue.full():
                time.sleep(0.001)
                continue
            ret, frame = self.cap.read()
            if not ret:
                break
            self.queue.put((frame_idx, frame))
            frame_idx += 1
        self.queue.put(None)

    def stop(self):
        self.stopped.set()


class FrameWriter(Thread):
    """ë¹„ë™ê¸° í”„ë ˆì„ ì“°ê¸° ìŠ¤ë ˆë“œ"""

    def __init__(self, out, queue):
        super().__init__(daemon=False)  # daemon=Falseë¡œ ë³€ê²½í•˜ì—¬ ì•ˆì „í•œ ì¢…ë£Œ
        self.out = out
        self.queue = queue
        self.stopped = Event()
        self.finished = Event()  # ì™„ë£Œ ì´ë²¤íŠ¸ ì¶”ê°€
        self.frames_written = 0

    def run(self):
        pending = {}
        next_frame = 0

        try:
            while not self.stopped.is_set():
                try:
                    item = self.queue.get(timeout=0.1)
                except Empty:
                    continue

                if item is None:
                    # ë‚¨ì€ pending í”„ë ˆì„ ëª¨ë‘ ì“°ê¸°
                    while next_frame in pending:
                        self.out.write(pending.pop(next_frame))
                        self.frames_written += 1
                        next_frame += 1
                    break

                frame_idx, frame = item
                pending[frame_idx] = frame

                while next_frame in pending:
                    self.out.write(pending.pop(next_frame))
                    self.frames_written += 1
                    next_frame += 1
        finally:
            self.finished.set()  # ì™„ë£Œ í‘œì‹œ

    def stop(self):
        self.stopped.set()

    def wait_finished(self, timeout=60):
        """ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°"""
        return self.finished.wait(timeout=timeout)


# ============================================================
# íŠ¸ë˜í‚¹ ë³´ê°„ê¸°
# ============================================================

class TrackingInterpolator:
    """
    íŠ¸ë˜í‚¹ ê²°ê³¼ ë³´ê°„ ê´€ë¦¬ (ê°œì„ ëœ ë²„ì „)
    - ê°ì§€ê°€ ëˆ„ë½ëœ í”„ë ˆì„ì—ì„œë„ ë§ˆì§€ë§‰ ìœ„ì¹˜ ìœ ì§€
    - ë¶€ë“œëŸ¬ìš´ ì†ë„ ê¸°ë°˜ ì˜ˆì¸¡
    - ê¹œë¹¡ì„ ë°©ì§€ë¥¼ ìœ„í•œ ì ê·¹ì ì¸ ë³´ê°„
    """

    def __init__(self, max_age=30, velocity_smoothing=0.5):
        self.tracks = {}
        self.max_age = max_age
        self.velocity_smoothing = velocity_smoothing  # ì†ë„ ì˜ˆì¸¡ ì œí•œ ë¹„ìœ¨
        self._next_temp_id = -1  # track_idê°€ Noneì¸ ê²½ìš° ì„ì‹œ ID

    def update(self, frame_idx, detections):
        """ìƒˆ ê°ì§€ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸"""
        seen_ids = set()

        for det in detections:
            track_id = det.get('track_id')

            # track_idê°€ Noneì¸ ê²½ìš°ì—ë„ ì²˜ë¦¬ (ìœ„ì¹˜ ê¸°ë°˜ ë§¤ì¹­ ì‹œë„)
            if track_id is None:
                box = det['box']
                matched_id = self._find_matching_track(box, frame_idx)
                if matched_id is not None:
                    track_id = matched_id
                else:
                    # ìƒˆë¡œìš´ ì„ì‹œ ID í• ë‹¹
                    track_id = self._next_temp_id
                    self._next_temp_id -= 1

            seen_ids.add(track_id)
            box = det['box']

            if track_id not in self.tracks:
                self.tracks[track_id] = {
                    'boxes': deque(maxlen=30),  # ë” ë§ì€ ì´ë ¥ ìœ ì§€
                    'last_seen': frame_idx,
                    'type': det.get('type', 'unknown'),
                    'velocity': (0, 0, 0, 0),  # ì†ë„ ì €ì¥
                    'conf': det.get('conf', 0.5)
                }

            # ì†ë„ ê³„ì‚° ë° ìŠ¤ë¬´ë”©
            if len(self.tracks[track_id]['boxes']) > 0:
                prev_frame, prev_box = self.tracks[track_id]['boxes'][-1]
                dt = frame_idx - prev_frame
                if dt > 0 and dt <= 10:  # í•©ë¦¬ì ì¸ í”„ë ˆì„ ê°„ê²©ì—ì„œë§Œ
                    new_velocity = tuple((b2 - b1) / dt for b1, b2 in zip(prev_box, box))
                    old_velocity = self.tracks[track_id]['velocity']
                    # ìŠ¤ë¬´ë”©ëœ ì†ë„
                    smoothed_velocity = tuple(
                        old_v * 0.7 + new_v * 0.3
                        for old_v, new_v in zip(old_velocity, new_velocity)
                    )
                    self.tracks[track_id]['velocity'] = smoothed_velocity

            self.tracks[track_id]['boxes'].append((frame_idx, box))
            self.tracks[track_id]['last_seen'] = frame_idx
            self.tracks[track_id]['conf'] = det.get('conf', self.tracks[track_id]['conf'])

        expired = [tid for tid, t in self.tracks.items()
                   if frame_idx - t['last_seen'] > self.max_age]
        for tid in expired:
            del self.tracks[tid]

    def _find_matching_track(self, box, frame_idx, iou_thresh=0.3):
        """ìœ„ì¹˜ ê¸°ë°˜ìœ¼ë¡œ ê¸°ì¡´ íŠ¸ë™ê³¼ ë§¤ì¹­"""
        best_match = None
        best_iou = iou_thresh

        for track_id, track in self.tracks.items():
            if frame_idx - track['last_seen'] > 10:  # ë„ˆë¬´ ì˜¤ë˜ëœ íŠ¸ë™ì€ ì œì™¸
                continue

            if len(track['boxes']) == 0:
                continue

            _, last_box = track['boxes'][-1]
            iou = self._calculate_iou(box, last_box)

            if iou > best_iou:
                best_iou = iou
                best_match = track_id

        return best_match

    def _calculate_iou(self, box1, box2):
        """IoU ê³„ì‚°"""
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])

        if x2 <= x1 or y2 <= y1:
            return 0.0

        intersection = (x2 - x1) * (y2 - y1)
        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
        union = area1 + area2 - intersection

        return intersection / union if union > 0 else 0.0

    def get_interpolated(self, frame_idx):
        """í˜„ì¬ í”„ë ˆì„ì— ëŒ€í•œ ë³´ê°„ëœ ë°•ìŠ¤ë“¤ ë°˜í™˜ (ê°œì„ ëœ ë²„ì „)"""
        results = []

        for track_id, track in self.tracks.items():
            boxes = track['boxes']
            if len(boxes) == 0:
                continue

            last_frame, last_box = boxes[-1]
            frames_gap = frame_idx - last_frame

            # í˜„ì¬ í”„ë ˆì„ì— ê°ì§€ëœ ê²½ìš°
            if frames_gap == 0:
                results.append({
                    'track_id': track_id,
                    'box': last_box,
                    'type': track['type'],
                    'interpolated': False
                })
            # ê°ì§€ê°€ ëˆ„ë½ë˜ì—ˆì§€ë§Œ max_age ì´ë‚´ì¸ ê²½ìš° - ë³´ê°„
            elif frames_gap > 0 and frames_gap <= self.max_age:
                velocity = track.get('velocity', (0, 0, 0, 0))

                # ì†ë„ ê¸°ë°˜ ì˜ˆì¸¡ (ì œí•œëœ ë²”ìœ„ ë‚´ì—ì„œ)
                max_shift = 50 * frames_gap  # í”„ë ˆì„ë‹¹ ìµœëŒ€ 50í”½ì…€ ì´ë™
                predicted_box = []
                for i, (coord, vel) in enumerate(zip(last_box, velocity)):
                    shift = vel * frames_gap
                    # ì´ë™ëŸ‰ ì œí•œ
                    shift = max(-max_shift, min(max_shift, shift))
                    predicted_box.append(int(coord + shift))

                # ë°•ìŠ¤ í¬ê¸°ê°€ ë„ˆë¬´ ë³€í•˜ì§€ ì•Šë„ë¡ ë³´ì •
                orig_w = last_box[2] - last_box[0]
                orig_h = last_box[3] - last_box[1]
                new_w = predicted_box[2] - predicted_box[0]
                new_h = predicted_box[3] - predicted_box[1]

                # í¬ê¸°ê°€ 20% ì´ìƒ ë³€í•˜ë©´ ì›ë˜ í¬ê¸° ìœ ì§€
                if new_w < orig_w * 0.8 or new_w > orig_w * 1.2:
                    center_x = (predicted_box[0] + predicted_box[2]) // 2
                    predicted_box[0] = center_x - orig_w // 2
                    predicted_box[2] = center_x + orig_w // 2
                if new_h < orig_h * 0.8 or new_h > orig_h * 1.2:
                    center_y = (predicted_box[1] + predicted_box[3]) // 2
                    predicted_box[1] = center_y - orig_h // 2
                    predicted_box[3] = center_y + orig_h // 2

                results.append({
                    'track_id': track_id,
                    'box': tuple(predicted_box),
                    'type': track['type'],
                    'interpolated': True
                })

        return results

    def get_all_active_tracks(self, frame_idx):
        """í™œì„±í™”ëœ ëª¨ë“  íŠ¸ë™ ìˆ˜ ë°˜í™˜ (ë””ë²„ê¹…ìš©)"""
        return sum(1 for t in self.tracks.values()
                   if frame_idx - t['last_seen'] <= self.max_age)


class PlateTracker:
    """
    ì°¨ëŸ‰ë³„ ë²ˆí˜¸íŒ ìœ„ì¹˜ íŠ¸ë˜í‚¹ ë° ë³´ê°„
    - ì°¨ëŸ‰ track_idë³„ë¡œ ë²ˆí˜¸íŒ ìœ„ì¹˜ë¥¼ ìºì‹±
    - ê°ì§€ ì‹¤íŒ¨ ì‹œ ì´ì „ ìœ„ì¹˜ ì‚¬ìš©
    - ë¶€ë“œëŸ¬ìš´ ë³´ê°„ìœ¼ë¡œ ê¹œë¹¡ì„ ë°©ì§€
    """
    
    def __init__(self, max_age=60, smoothing=0.7):
        """
        Args:
            max_age: ë²ˆí˜¸íŒì´ ê°ì§€ë˜ì§€ ì•Šì•„ë„ ìœ ì§€í•  ìµœëŒ€ í”„ë ˆì„ ìˆ˜
            smoothing: ìœ„ì¹˜ ìŠ¤ë¬´ë”© ë¹„ìœ¨ (0~1, ë†’ì„ìˆ˜ë¡ ì´ì „ ìœ„ì¹˜ ìœ ì§€)
        """
        self.plate_cache = {}  # vehicle_track_id -> {'plates': [...], 'last_seen': frame_idx}
        self.max_age = max_age
        self.smoothing = smoothing
    
    def update(self, frame_idx, vehicle_track_id, plate_regions):
        """
        ì°¨ëŸ‰ì˜ ë²ˆí˜¸íŒ ìœ„ì¹˜ ì—…ë°ì´íŠ¸
        
        Args:
            frame_idx: í˜„ì¬ í”„ë ˆì„ ì¸ë±ìŠ¤
            vehicle_track_id: ì°¨ëŸ‰ íŠ¸ë˜í‚¹ ID
            plate_regions: ê°ì§€ëœ ë²ˆí˜¸íŒ ì˜ì—­ë“¤ [(x1,y1,x2,y2), ...]
        """
        if vehicle_track_id is None:
            return
        
        if plate_regions and len(plate_regions) > 0:
            # ìƒˆë¡œìš´ ë²ˆí˜¸íŒ ê°ì§€ë¨
            if vehicle_track_id in self.plate_cache:
                # ê¸°ì¡´ ìœ„ì¹˜ì™€ ìŠ¤ë¬´ë”©
                old_plates = self.plate_cache[vehicle_track_id]['plates']
                smoothed_plates = []
                
                for i, new_plate in enumerate(plate_regions):
                    if i < len(old_plates):
                        # ì´ì „ ìœ„ì¹˜ì™€ ìƒˆ ìœ„ì¹˜ë¥¼ ìŠ¤ë¬´ë”©
                        old = old_plates[i]
                        smoothed = tuple(
                            int(old[j] * self.smoothing + new_plate[j] * (1 - self.smoothing))
                            for j in range(4)
                        )
                        smoothed_plates.append(smoothed)
                    else:
                        smoothed_plates.append(new_plate)
                
                self.plate_cache[vehicle_track_id] = {
                    'plates': smoothed_plates,
                    'last_seen': frame_idx,
                    'detected': True
                }
            else:
                # ìƒˆë¡œìš´ ì°¨ëŸ‰
                self.plate_cache[vehicle_track_id] = {
                    'plates': list(plate_regions),
                    'last_seen': frame_idx,
                    'detected': True
                }
        else:
            # ë²ˆí˜¸íŒ ê°ì§€ ì‹¤íŒ¨ - ì´ì „ ìœ„ì¹˜ ìœ ì§€
            if vehicle_track_id in self.plate_cache:
                self.plate_cache[vehicle_track_id]['detected'] = False
    
    def get_plates(self, frame_idx, vehicle_track_id, vehicle_box=None):
        """
        ì°¨ëŸ‰ì˜ ë²ˆí˜¸íŒ ìœ„ì¹˜ ë°˜í™˜ (ìºì‹œëœ ìœ„ì¹˜ ë˜ëŠ” ì¶”ì • ìœ„ì¹˜)
        
        Args:
            frame_idx: í˜„ì¬ í”„ë ˆì„ ì¸ë±ìŠ¤
            vehicle_track_id: ì°¨ëŸ‰ íŠ¸ë˜í‚¹ ID
            vehicle_box: í˜„ì¬ ì°¨ëŸ‰ ë°•ìŠ¤ (ìºì‹œê°€ ì—†ì„ ë•Œ ìœ„ì¹˜ ì¶”ì •ìš©)
        
        Returns:
            [(x1, y1, x2, y2), ...] ë²ˆí˜¸íŒ ì˜ì—­ë“¤
        """
        if vehicle_track_id is None:
            return []
        
        if vehicle_track_id in self.plate_cache:
            cache = self.plate_cache[vehicle_track_id]
            age = frame_idx - cache['last_seen']
            
            if age <= self.max_age:
                return cache['plates']
        
        return []
    
    def cleanup(self, frame_idx):
        """ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬"""
        expired = [
            tid for tid, cache in self.plate_cache.items()
            if frame_idx - cache['last_seen'] > self.max_age
        ]
        for tid in expired:
            del self.plate_cache[tid]


# ============================================================
# ê¸°ë³¸ ë§ˆìŠ¤í‚¹ í´ë˜ìŠ¤
# ============================================================

class VideoMasker:
    """ê¸°ë³¸ ë¹„ë””ì˜¤ ë§ˆìŠ¤í‚¹ í´ë˜ìŠ¤ (ê°„ë‹¨í•˜ê³  ë¹ ë¥¸ ì„¤ì •ìš©)"""
    
    # COCO í´ë˜ìŠ¤ ID
    PERSON_CLASS = 0
    VEHICLE_CLASSES = [2, 3, 5, 7]  # car, motorcycle, bus, truck

    def __init__(
        self,
        mask_persons: bool = True,
        mask_plates: bool = True,
        mask_type: str = "blur",
        blur_strength: int = 51,
        mosaic_size: int = 15,
        person_confidence: float = 0.4,
        vehicle_confidence: float = 0.3,
        person_expand: float = 0.1,
        plate_expand: float = 0.3,
        plate_detect_mode: str = "auto",  # "auto", "multi", "legacy"
        max_mask_ratio: float = 0.4,  # í”„ë ˆì„ ëŒ€ë¹„ ìµœëŒ€ ë§ˆìŠ¤í‚¹ ì˜ì—­ ë¹„ìœ¨
    ):
        self.mask_persons = mask_persons
        self.mask_plates = mask_plates
        self.mask_type = mask_type
        self.blur_strength = blur_strength
        self.mosaic_size = mosaic_size
        self.person_confidence = person_confidence
        self.vehicle_confidence = vehicle_confidence
        self.person_expand = person_expand
        self.plate_expand = plate_expand
        self.plate_detect_mode = plate_detect_mode
        self.max_mask_ratio = max_mask_ratio  # ë¹„ì •ìƒì ìœ¼ë¡œ í° ë§ˆìŠ¤í‚¹ ë°©ì§€

        # ëª¨ë¸ ë¡œë“œ
        self.yolo_model = None
        if mask_persons or mask_plates:
            print("YOLOv8 ëª¨ë¸ ë¡œë”©...")
            self.yolo_model = YOLO("yolov8n.pt")
            print(f"   âœ… ì‚¬ëŒ: {'O' if mask_persons else 'X'}, ë²ˆí˜¸íŒ: {'O' if mask_plates else 'X'}")
            if mask_plates:
                print(f"   ğŸ“ ë²ˆí˜¸íŒ ê°ì§€ ëª¨ë“œ: {plate_detect_mode}")
            print(f"   ğŸ›¡ï¸ ìµœëŒ€ ë§ˆìŠ¤í‚¹ ë¹„ìœ¨: {max_mask_ratio*100:.0f}%")

    def apply_mask(self, frame, x1, y1, x2, y2):
        """ë§ˆìŠ¤í¬ ì ìš©"""
        if self.mask_type == "blur":
            return apply_blur(frame, x1, y1, x2, y2, self.blur_strength)
        else:
            return apply_mosaic(frame, x1, y1, x2, y2, self.mosaic_size)

    def safe_apply_mask(self, frame, x1, y1, x2, y2, max_ratio=None):
        """
        ì•ˆì „í•œ ë§ˆìŠ¤í¬ ì ìš© (ë¹„ì •ìƒì ìœ¼ë¡œ í° ì˜ì—­ í•„í„°ë§)
        
        Returns:
            frame: ë§ˆìŠ¤í¬ ì ìš©ëœ í”„ë ˆì„ (ë˜ëŠ” ì›ë³¸)
            bool: ë§ˆìŠ¤í¬ê°€ ì ìš©ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€
        """
        if max_ratio is None:
            max_ratio = self.max_mask_ratio
        
        # ë°•ìŠ¤ ê²€ì¦ ë° í´ë¦¬í•‘
        validated_box = validate_and_clip_box(
            (x1, y1, x2, y2), 
            frame.shape, 
            max_area_ratio=max_ratio,
            min_size=10
        )
        
        if validated_box is None:
            # ë¹„ì •ìƒì ì¸ ë°•ìŠ¤ - ë§ˆìŠ¤í‚¹ ìŠ¤í‚µ
            return frame, False
        
        x1, y1, x2, y2 = validated_box
        return self.apply_mask(frame, x1, y1, x2, y2), True

    def process_frame(self, frame):
        """ë‹¨ì¼ í”„ë ˆì„ ì²˜ë¦¬"""
        if not self.yolo_model:
            return frame

        classes_to_detect = []
        if self.mask_persons:
            classes_to_detect.append(self.PERSON_CLASS)
        if self.mask_plates:
            classes_to_detect.extend(self.VEHICLE_CLASSES)

        results = self.yolo_model.track(
            frame,
            persist=True,
            conf=min(self.person_confidence, self.vehicle_confidence),
            classes=classes_to_detect,
            verbose=False
        )

        if results[0].boxes is not None:
            for box in results[0].boxes:
                cls = int(box.cls[0])
                conf = float(box.conf[0])
                xyxy = box.xyxy[0].cpu().numpy()

                # ì‚¬ëŒ ë§ˆìŠ¤í‚¹ (ì•ˆì „í•œ ì ìš©)
                if self.mask_persons and cls == self.PERSON_CLASS and conf >= self.person_confidence:
                    x1, y1, x2, y2 = expand_box(xyxy, self.person_expand, frame.shape)
                    frame, _ = self.safe_apply_mask(frame, x1, y1, x2, y2)

                # ë²ˆí˜¸íŒ ë§ˆìŠ¤í‚¹ (ê°œì„ ëœ ë°©ì‹ + ì•ˆì „í•œ ì ìš©)
                if self.mask_plates and cls in self.VEHICLE_CLASSES and conf >= self.vehicle_confidence:
                    if self.plate_detect_mode == "legacy":
                        # ê¸°ì¡´ ë°©ì‹: í•˜ë‹¨ ì˜ì—­ë§Œ
                        px1, py1, px2, py2 = get_plate_region(xyxy, self.plate_expand)
                        frame, _ = self.safe_apply_mask(frame, px1, py1, px2, py2, max_ratio=0.15)
                    else:
                        # ê°œì„ ëœ ë°©ì‹: OpenCV ê°ì§€ ë˜ëŠ” ë‹¤ì¤‘ ìœ„ì¹˜
                        use_detection = (self.plate_detect_mode == "auto")
                        plate_regions = get_all_plate_regions(
                            frame, xyxy, 
                            expand_ratio=self.plate_expand,
                            use_detection=use_detection
                        )
                        for px1, py1, px2, py2 in plate_regions:
                            # ë²ˆí˜¸íŒì€ í”„ë ˆì„ì˜ 15% ì´í•˜ì—¬ì•¼ í•¨
                            frame, _ = self.safe_apply_mask(frame, px1, py1, px2, py2, max_ratio=0.15)

        return frame

    def process_video(
        self,
        input_path: str,
        output_path: str = None,
        start_time: float = None,
        end_time: float = None,
        max_frames: int = None,
        preview: bool = False,
        preview_scale: float = 0.5,
        use_hevc: bool = False,
        log_file: str = None,
        verbose: bool = False,
    ):
        """ë¹„ë””ì˜¤ ì „ì²´ ì²˜ë¦¬"""
        # ë¡œê±° ì„¤ì •
        if log_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            log_file = str(Path(input_path).parent / f"masking_{timestamp}.log")

        logger = setup_logger(log_file, verbose)
        logger.info("=" * 60)
        logger.info("ë§ˆìŠ¤í‚¹ ì‘ì—… ì‹œì‘ (ê¸°ë³¸ ëª¨ë“œ)")
        logger.info("=" * 60)

        start_total_time = time.time()

        cap = cv2.VideoCapture(input_path)
        if not cap.isOpened():
            raise ValueError(f"ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")

        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        start_frame = int(start_time * fps) if start_time else 0
        end_frame = int(end_time * fps) if end_time else total_frames
        end_frame = min(end_frame, total_frames)
        if max_frames:
            end_frame = min(start_frame + max_frames, end_frame)
        process_frames = end_frame - start_frame

        logger.info(f"ì…ë ¥ íŒŒì¼: {input_path}")
        logger.info(f"í•´ìƒë„: {width}x{height}, FPS: {fps:.2f}")
        logger.info(f"ì²˜ë¦¬ í”„ë ˆì„: {process_frames}")

        if output_path is None:
            input_stem = Path(input_path).stem
            suffix = f"_{int(start_time//60)}m-{int(end_time//60)}m" if start_time else ""
            output_path = str(Path(input_path).parent / f"{input_stem}{suffix}_masked.mp4")

        if start_frame > 0:
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)

        # HEVC ì‚¬ìš©ì‹œ ì„ì‹œ íŒŒì¼
        if use_hevc:
            temp_output = tempfile.NamedTemporaryFile(suffix='.mp4', delete=False).name
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(temp_output, fourcc, fps, (width, height))
            logger.info(f"ì¶œë ¥ íŒŒì¼: {output_path} (HEVC ì˜ˆì •)")
        else:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            logger.info(f"ì¶œë ¥ íŒŒì¼: {output_path}")

        logger.info(f"ì„¤ì •: ì‚¬ëŒ={'O' if self.mask_persons else 'X'}, "
                   f"ë²ˆí˜¸íŒ={'O' if self.mask_plates else 'X'}, ë°©ì‹={self.mask_type}")
        logger.info("-" * 60)

        frame_count = 0
        errors = []

        try:
            while frame_count < process_frames:
                ret, frame = cap.read()
                if not ret:
                    break

                frame_count += 1

                try:
                    frame = self.process_frame(frame)
                    out.write(frame)
                except Exception as e:
                    logger.error(f"í”„ë ˆì„ {frame_count} ì˜¤ë¥˜: {e}")
                    errors.append(str(e))
                    out.write(frame)

                if frame_count % 60 == 0:
                    progress = frame_count / process_frames * 100
                    elapsed = time.time() - start_total_time
                    avg_fps = frame_count / elapsed if elapsed > 0 else 0
                    logger.info(f"[{progress:5.1f}%] {frame_count}/{process_frames} | {avg_fps:.1f} fps")

                if preview:
                    display = cv2.resize(frame, (int(width * preview_scale), int(height * preview_scale)))
                    cv2.imshow("Preview (Q to quit)", display)
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        logger.warning("ì‚¬ìš©ì ì¤‘ë‹¨")
                        break

        finally:
            cap.release()
            out.release()
            if preview:
                cv2.destroyAllWindows()

        # HEVC ì¸ì½”ë”©
        if use_hevc:
            logger.info("HEVC ì¸ì½”ë”© ì‹œì‘...")
            ffmpeg_cmd = [
                'ffmpeg', '-y', '-i', temp_output,
                '-c:v', 'libx265', '-preset', 'medium', '-crf', '23',
                '-tag:v', 'hvc1', '-an', output_path
            ]
            try:
                result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)
                if result.returncode == 0:
                    logger.info("HEVC ì¸ì½”ë”© ì™„ë£Œ!")
                    os.unlink(temp_output)
                else:
                    logger.error(f"HEVC ì‹¤íŒ¨: {result.stderr}")
            except Exception as e:
                logger.error(f"ffmpeg ì˜¤ë¥˜: {e}")

        total_time = time.time() - start_total_time
        logger.info("=" * 60)
        logger.info(f"ì™„ë£Œ! ì¶œë ¥: {output_path}")
        logger.info(f"ì†Œìš”ì‹œê°„: {total_time/60:.1f}ë¶„, ì˜¤ë¥˜: {len(errors)}")
        logger.info("=" * 60)

        return output_path


# ============================================================
# ìµœì í™”ëœ ë§ˆìŠ¤í‚¹ í´ë˜ìŠ¤
# ============================================================

class VideoMaskerOptimized(VideoMasker):
    """ìµœì í™”ëœ ë¹„ë””ì˜¤ ë§ˆìŠ¤í‚¹ í´ë˜ìŠ¤ (GPU ê°€ì†, ë©€í‹°ìŠ¤ë ˆë”©, íŠ¸ë˜í‚¹ ë³´ê°„)"""

    def __init__(
        self,
        mask_persons: bool = True,
        mask_plates: bool = True,
        mask_type: str = "blur",
        blur_strength: int = 51,
        mosaic_size: int = 15,
        person_confidence: float = 0.4,
        vehicle_confidence: float = 0.3,
        person_expand: float = 0.1,
        plate_expand: float = 0.3,
        plate_detect_mode: str = "auto",  # "auto", "multi", "legacy"
        plate_smoothing: float = 0.6,  # ë²ˆí˜¸íŒ ìœ„ì¹˜ ìŠ¤ë¬´ë”© (0~1, ë†’ì„ìˆ˜ë¡ ì•ˆì •ì )
        max_mask_ratio: float = 0.4,  # í”„ë ˆì„ ëŒ€ë¹„ ìµœëŒ€ ë§ˆìŠ¤í‚¹ ì˜ì—­ ë¹„ìœ¨
        # ìµœì í™” íŒŒë¼ë¯¸í„°
        device: str = "auto",
        detect_interval: int = -1,
        detect_scale: float = -1,
        batch_size: int = -1,
        # NVIDIA GPU ìµœì í™”
        gpu_id: int = 0,
        use_fp16: bool = None,
        use_tensorrt: bool = False,
        yolo_model: str = "yolov8n",  # yolov8n, yolov8s, yolov8m, yolov8l, yolov8x
        # ì‹œìŠ¤í…œ ìµœì í™”
        auto_optimize: bool = True,
        queue_size: int = -1,
        high_performance: bool = False,  # ê³ ì„±ëŠ¥ ëª¨ë“œ (FFmpeg íŒŒì´í”„ë¼ì¸)
        # íŠ¸ë˜í‚¹ íŒŒë¼ë¯¸í„° (track_buffer: ê°ì§€ ëˆ„ë½ ì‹œ ìœ ì§€í•  í”„ë ˆì„ ìˆ˜)
        tracker: str = "bytetrack",
        track_buffer: int = 60,  # ê¸°ë³¸ê°’ ì¦ê°€: ê¹œë¹¡ì„ ë°©ì§€
        match_thresh: float = 0.8,
        iou_thresh: float = 0.5,
        # í•˜ë“œì›¨ì–´ ê°€ì† ë””ì½”ë”©
        use_nvdec: bool = True,  # NVDEC GPU ë””ì½”ë”© (NVIDIA GPU í•„ìˆ˜)
    ):
        # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ ê¸°ë³¸ ì†ì„±ë§Œ ì„¤ì • (ëª¨ë¸ ë¡œë“œ ì œì™¸)
        self.mask_persons = mask_persons
        self.mask_plates = mask_plates
        self.mask_type = mask_type
        self.blur_strength = blur_strength
        self.mosaic_size = mosaic_size
        self.person_confidence = person_confidence
        self.vehicle_confidence = vehicle_confidence
        self.person_expand = person_expand
        self.plate_expand = plate_expand
        self.plate_detect_mode = plate_detect_mode
        self.plate_smoothing = plate_smoothing
        self.max_mask_ratio = max_mask_ratio
        
        # ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ ë° ìë™ ìµœì í™”
        self.system_info = None
        self.optimal_settings = None
        
        if auto_optimize:
            self.system_info = get_system_info()
            self.optimal_settings = get_optimal_settings(self.system_info)
            print_system_info(self.system_info, self.optimal_settings)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device == "auto":
            self.device = self.optimal_settings['device'] if self.optimal_settings else get_optimal_device()
        else:
            self.device = device

        # ìµœì í™” ì„¤ì •
        if self.optimal_settings:
            self.detect_interval = detect_interval if detect_interval > 0 else self.optimal_settings['detect_interval']
            self.detect_scale = detect_scale if detect_scale > 0 else self.optimal_settings['detect_scale']
            self.batch_size = batch_size if batch_size > 0 else self.optimal_settings['batch_size']
            self.queue_size = queue_size if queue_size > 0 else self.optimal_settings['queue_size']
            self.use_fp16 = use_fp16 if use_fp16 is not None else self.optimal_settings['use_fp16']
            self.num_workers = self.optimal_settings['num_workers']
        else:
            self.detect_interval = detect_interval if detect_interval > 0 else 3
            self.detect_scale = detect_scale if detect_scale > 0 else 0.5
            self.batch_size = batch_size if batch_size > 0 else 4
            self.queue_size = queue_size if queue_size > 0 else 128
            self.use_fp16 = use_fp16 if use_fp16 is not None else False
            self.num_workers = 2
        
        # ê³ ì„±ëŠ¥ ëª¨ë“œ (FFmpeg íŒŒì´í”„ë¼ì¸ + ì§„ì •í•œ ë°°ì¹˜ ì¶”ë¡ )
        self.high_performance = high_performance and self.device == 'cuda'
        if self.high_performance:
            # ê³ ì„±ëŠ¥ ëª¨ë“œì—ì„œëŠ” ë°°ì¹˜ í¬ê¸°ì™€ í í¬ê¸° ì¦ê°€
            self.batch_size = max(self.batch_size, 8)
            self.queue_size = max(self.queue_size, 256)
            print(f"   ğŸš€ ê³ ì„±ëŠ¥ ëª¨ë“œ í™œì„±í™” (ë°°ì¹˜: {self.batch_size}, í: {self.queue_size})")
        
        self.tracker_type = tracker
        self.track_buffer = track_buffer
        self.match_thresh = match_thresh
        self.iou_thresh = iou_thresh
        self.gpu_id = gpu_id
        self.use_tensorrt = use_tensorrt

        # NVDEC ì„¤ì • (CUDA ë””ë°”ì´ìŠ¤ì¼ ë•Œë§Œ ì‚¬ìš© ê°€ëŠ¥)
        self.use_nvdec = use_nvdec and self.device == 'cuda'
        if self.use_nvdec:
            print(f"   ğŸ¬ NVDEC GPU ë””ì½”ë”© í™œì„±í™”")

        # CUDA ìµœì í™”
        if self.device == 'cuda':
            setup_cuda_optimization(self.device, gpu_id)
            if self.use_fp16:
                print("   âš¡ FP16 ë°˜ì •ë°€ë„ ì¶”ë¡  í™œì„±í™”")

        # íŠ¸ë˜ì»¤ ì„¤ì •
        self.tracker_config_path = create_custom_tracker_config(tracker, track_buffer, match_thresh)
        print(f"íŠ¸ë˜ì»¤: {tracker} (buffer={track_buffer})")

        # ëª¨ë¸ ë¡œë“œ
        self.yolo_model = None
        self.yolo_half = False

        self.yolo_model_name = yolo_model

        if mask_persons or mask_plates:
            model_file = f"{yolo_model}.pt"
            print(f"YOLO ëª¨ë¸ ë¡œë”©: {model_file} (device={self.device})...")

            if use_tensorrt and self.device == 'cuda':
                tensorrt_path = Path(f"{yolo_model}.engine")
                if tensorrt_path.exists():
                    print(f"   TensorRT ì—”ì§„ ë¡œë”©: {tensorrt_path}")
                    self.yolo_model = YOLO(str(tensorrt_path), task='detect')
                else:
                    print(f"   âš ï¸ TensorRT ì—”ì§„ ì—†ìŒ, PyTorch ëª¨ë¸ ì‚¬ìš©")
                    self.yolo_model = YOLO(model_file)
            else:
                self.yolo_model = YOLO(model_file)

            # TensorRT ëª¨ë¸ì€ .to() í˜¸ì¶œ ë¶ˆê°€ (ì´ë¯¸ GPUì— ì»´íŒŒì¼ë¨)
            if self.device != 'cpu' and not self.use_tensorrt:
                self.yolo_model.to(self.device)

            self.yolo_half = self.use_fp16 and self.device == 'cuda' and not self.use_tensorrt
            print(f"   âœ… ì‚¬ëŒ: {'O' if mask_persons else 'X'}, ë²ˆí˜¸íŒ: {'O' if mask_plates else 'X'}")

        # íŠ¸ë˜í‚¹ ë³´ê°„ê¸°
        self.person_interpolator = TrackingInterpolator(max_age=track_buffer)
        self.vehicle_interpolator = TrackingInterpolator(max_age=track_buffer)
        
        # ë²ˆí˜¸íŒ íŠ¸ë˜ì»¤ (ê¹œë¹¡ì„ ë°©ì§€)
        self.plate_tracker = PlateTracker(max_age=track_buffer * 2, smoothing=self.plate_smoothing)

    def detect_all(self, frame, frame_idx):
        """ì‚¬ëŒê³¼ ì°¨ëŸ‰ ëª¨ë‘ ê°ì§€"""
        detections = {'persons': [], 'vehicles': []}
        h, w = frame.shape[:2]

        if not self.yolo_model:
            return detections

        # ë‹¤ìš´ìŠ¤ì¼€ì¼
        if self.detect_scale < 1.0:
            detect_frame = cv2.resize(frame, (int(w * self.detect_scale), int(h * self.detect_scale)))
            scale_x = w / detect_frame.shape[1]
            scale_y = h / detect_frame.shape[0]
        else:
            detect_frame = frame
            scale_x = scale_y = 1.0

        classes_to_detect = []
        if self.mask_persons:
            classes_to_detect.append(self.PERSON_CLASS)
        if self.mask_plates:
            classes_to_detect.extend(self.VEHICLE_CLASSES)

        results = self.yolo_model.track(
            detect_frame,
            persist=True,
            conf=min(self.person_confidence, self.vehicle_confidence),
            iou=self.iou_thresh,
            classes=classes_to_detect,
            tracker=self.tracker_config_path,
            device=self.device,
            half=self.yolo_half,
            verbose=False
        )

        if results[0].boxes is not None:
            for box in results[0].boxes:
                cls = int(box.cls[0])
                conf = float(box.conf[0])
                xyxy = box.xyxy[0].cpu().numpy()
                
                x1, y1, x2, y2 = xyxy
                x1, x2 = int(x1 * scale_x), int(x2 * scale_x)
                y1, y2 = int(y1 * scale_y), int(y2 * scale_y)
                
                track_id = int(box.id[0]) if box.id is not None else None

                if self.mask_persons and cls == self.PERSON_CLASS and conf >= self.person_confidence:
                    detections['persons'].append({
                        'box': (x1, y1, x2, y2), 'track_id': track_id,
                        'type': 'person', 'conf': conf
                    })
                
                if self.mask_plates and cls in self.VEHICLE_CLASSES and conf >= self.vehicle_confidence:
                    detections['vehicles'].append({
                        'box': (x1, y1, x2, y2), 'track_id': track_id,
                        'type': 'vehicle', 'class': cls
                    })

        return detections

    def detect_batch(self, frames, frame_indices):
        """
        ì—¬ëŸ¬ í”„ë ˆì„ì— ëŒ€í•´ ì§„ì •í•œ ë°°ì¹˜ ì¶”ë¡  ìˆ˜í–‰
        GPU í™œìš©ë¥  ê·¹ëŒ€í™”ë¥¼ ìœ„í•œ í•µì‹¬ ë©”ì„œë“œ
        """
        if not self.yolo_model or not frames:
            return [{} for _ in frames]
        
        h, w = frames[0].shape[:2]
        
        # ë‹¤ìš´ìŠ¤ì¼€ì¼
        if self.detect_scale < 1.0:
            detect_frames = [
                cv2.resize(f, (int(w * self.detect_scale), int(h * self.detect_scale))) 
                for f in frames
            ]
            scale_x = w / detect_frames[0].shape[1]
            scale_y = h / detect_frames[0].shape[0]
        else:
            detect_frames = frames
            scale_x = scale_y = 1.0
        
        classes_to_detect = []
        if self.mask_persons:
            classes_to_detect.append(self.PERSON_CLASS)
        if self.mask_plates:
            classes_to_detect.extend(self.VEHICLE_CLASSES)
        
        # ì§„ì •í•œ ë°°ì¹˜ ì¶”ë¡  - ëª¨ë“  í”„ë ˆì„ì„ í•œë²ˆì— GPUë¡œ ì „ì†¡
        results_list = self.yolo_model.track(
            detect_frames,
            persist=True,
            conf=min(self.person_confidence, self.vehicle_confidence),
            iou=self.iou_thresh,
            classes=classes_to_detect,
            tracker=self.tracker_config_path,
            device=self.device,
            half=self.yolo_half,
            verbose=False
        )
        
        all_detections = []
        for i, (results, frame_idx) in enumerate(zip(results_list, frame_indices)):
            detections = {'persons': [], 'vehicles': []}
            
            if results.boxes is not None:
                for box in results.boxes:
                    cls = int(box.cls[0])
                    conf = float(box.conf[0])
                    xyxy = box.xyxy[0].cpu().numpy()
                    
                    x1, y1, x2, y2 = xyxy
                    x1, x2 = int(x1 * scale_x), int(x2 * scale_x)
                    y1, y2 = int(y1 * scale_y), int(y2 * scale_y)
                    
                    track_id = int(box.id[0]) if box.id is not None else None
                    
                    if self.mask_persons and cls == self.PERSON_CLASS and conf >= self.person_confidence:
                        detections['persons'].append({
                            'box': (x1, y1, x2, y2), 'track_id': track_id,
                            'type': 'person', 'conf': conf
                        })
                    
                    if self.mask_plates and cls in self.VEHICLE_CLASSES and conf >= self.vehicle_confidence:
                        detections['vehicles'].append({
                            'box': (x1, y1, x2, y2), 'track_id': track_id,
                            'type': 'vehicle', 'class': cls
                        })
            
            all_detections.append(detections)
        
        return all_detections

    def process_batch_parallel(self, frames, frame_indices):
        """
        ë°°ì¹˜ í”„ë ˆì„ ì²˜ë¦¬ (ì§„ì •í•œ ë°°ì¹˜ ì¶”ë¡  + ë§ˆìŠ¤í‚¹)
        """
        # ê°ì§€ê°€ í•„ìš”í•œ í”„ë ˆì„ ì°¾ê¸°
        detect_needed = [
            (i, f, idx) for i, (f, idx) in enumerate(zip(frames, frame_indices))
            if idx % self.detect_interval == 0
        ]
        
        # ë°°ì¹˜ ê°ì§€ ìˆ˜í–‰
        if detect_needed:
            detect_frames = [f for _, f, _ in detect_needed]
            detect_indices = [idx for _, _, idx in detect_needed]
            batch_detections = self.detect_batch(detect_frames, detect_indices)
            
            # ë³´ê°„ê¸° ì—…ë°ì´íŠ¸
            for (i, _, idx), detections in zip(detect_needed, batch_detections):
                self.person_interpolator.update(idx, detections['persons'])
                self.vehicle_interpolator.update(idx, detections['vehicles'])
        
        # ëª¨ë“  í”„ë ˆì„ì— ë§ˆìŠ¤í‚¹ ì ìš©
        results = []
        total_persons = 0
        total_vehicles = 0
        
        for frame, frame_idx in zip(frames, frame_indices):
            persons = self.person_interpolator.get_interpolated(frame_idx)
            vehicles = self.vehicle_interpolator.get_interpolated(frame_idx)
            
            # ì‚¬ëŒ ë§ˆìŠ¤í‚¹
            if self.mask_persons:
                for person in persons:
                    box = expand_box(person['box'], self.person_expand, frame.shape)
                    frame, _ = self.safe_apply_mask(frame, *box)
                total_persons += len(persons)
            
            # ë²ˆí˜¸íŒ ë§ˆìŠ¤í‚¹
            if self.mask_plates:
                for vehicle in vehicles:
                    track_id = vehicle.get('track_id')
                    vehicle_box = vehicle['box']
                    
                    if self.plate_detect_mode == "legacy":
                        plate_box = get_plate_region(vehicle_box, self.plate_expand)
                        frame, _ = self.safe_apply_mask(frame, *plate_box, max_ratio=0.15)
                    else:
                        use_detection = (self.plate_detect_mode == "auto")
                        
                        if frame_idx % self.detect_interval == 0:
                            detected_plates = get_all_plate_regions(
                                frame, vehicle_box,
                                expand_ratio=self.plate_expand,
                                use_detection=use_detection
                            )
                            self.plate_tracker.update(frame_idx, track_id, detected_plates)
                        
                        plate_regions = self.plate_tracker.get_plates(frame_idx, track_id, vehicle_box)
                        
                        if not plate_regions:
                            plate_regions = get_all_plate_regions(
                                frame, vehicle_box,
                                expand_ratio=self.plate_expand,
                                use_detection=use_detection
                            )
                        
                        for plate_box in plate_regions:
                            frame, _ = self.safe_apply_mask(frame, *plate_box, max_ratio=0.15)
                
                total_vehicles += len(vehicles)
            
            results.append(frame)
        
        # ìºì‹œ ì •ë¦¬
        if frame_indices and frame_indices[-1] % 30 == 0:
            self.plate_tracker.cleanup(frame_indices[-1])
        
        return results, total_persons, total_vehicles

    def process_frame_optimized(self, frame, frame_idx, force_detect=False):
        """ë‹¨ì¼ í”„ë ˆì„ ì²˜ë¦¬ (ë³´ê°„ + ë²ˆí˜¸íŒ íŠ¸ë˜í‚¹ í¬í•¨)"""
        should_detect = force_detect or (frame_idx % self.detect_interval == 0)

        if should_detect:
            detections = self.detect_all(frame, frame_idx)
            self.person_interpolator.update(frame_idx, detections['persons'])
            self.vehicle_interpolator.update(frame_idx, detections['vehicles'])

        persons = self.person_interpolator.get_interpolated(frame_idx)
        vehicles = self.vehicle_interpolator.get_interpolated(frame_idx)

        # ì‚¬ëŒ ë§ˆìŠ¤í‚¹ (ì•ˆì „í•œ ì ìš©)
        if self.mask_persons:
            for person in persons:
                box = expand_box(person['box'], self.person_expand, frame.shape)
                frame, _ = self.safe_apply_mask(frame, *box)

        # ë²ˆí˜¸íŒ ë§ˆìŠ¤í‚¹ (íŠ¸ë˜í‚¹ + ë³´ê°„ìœ¼ë¡œ ê¹œë¹¡ì„ ë°©ì§€)
        if self.mask_plates:
            for vehicle in vehicles:
                track_id = vehicle.get('track_id')
                vehicle_box = vehicle['box']
                
                if self.plate_detect_mode == "legacy":
                    # ê¸°ì¡´ ë°©ì‹: í•˜ë‹¨ ì˜ì—­ë§Œ (íŠ¸ë˜í‚¹ ì—†ìŒ)
                    plate_box = get_plate_region(vehicle_box, self.plate_expand)
                    frame, _ = self.safe_apply_mask(frame, *plate_box, max_ratio=0.15)
                else:
                    # ê°œì„ ëœ ë°©ì‹: ë²ˆí˜¸íŒ íŠ¸ë˜í‚¹ + ë³´ê°„
                    plate_regions = []
                    
                    # ê°ì§€ê°€ í•„ìš”í•œ í”„ë ˆì„ì—ì„œë§Œ ìƒˆë¡œ ê°ì§€
                    if should_detect or not vehicle.get('interpolated', False):
                        use_detection = (self.plate_detect_mode == "auto")
                        detected_plates = get_all_plate_regions(
                            frame, vehicle_box,
                            expand_ratio=self.plate_expand,
                            use_detection=use_detection
                        )
                        
                        # ë²ˆí˜¸íŒ íŠ¸ë˜ì»¤ ì—…ë°ì´íŠ¸
                        self.plate_tracker.update(frame_idx, track_id, detected_plates)
                    
                    # ìºì‹œëœ ë²ˆí˜¸íŒ ìœ„ì¹˜ ì‚¬ìš© (ë¶€ë“œëŸ¬ìš´ ë³´ê°„)
                    plate_regions = self.plate_tracker.get_plates(frame_idx, track_id, vehicle_box)
                    
                    # ìºì‹œê°€ ì—†ìœ¼ë©´ í˜„ì¬ ê°ì§€ ê²°ê³¼ ì‚¬ìš©
                    if not plate_regions:
                        use_detection = (self.plate_detect_mode == "auto")
                        plate_regions = get_all_plate_regions(
                            frame, vehicle_box,
                            expand_ratio=self.plate_expand,
                            use_detection=use_detection
                        )
                    
                    for plate_box in plate_regions:
                        # ë²ˆí˜¸íŒì€ í”„ë ˆì„ì˜ 15% ì´í•˜ì—¬ì•¼ í•¨
                        frame, _ = self.safe_apply_mask(frame, *plate_box, max_ratio=0.15)
            
            # ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬ (ì£¼ê¸°ì ìœ¼ë¡œ)
            if frame_idx % 30 == 0:
                self.plate_tracker.cleanup(frame_idx)

        return frame, len(persons), len(vehicles)

    def process_video_high_performance(
        self,
        input_path: str,
        output_path: str,
        start_time: float = None,
        end_time: float = None,
        use_hevc: bool = False,
        logger = None
    ):
        """
        ê³ ì„±ëŠ¥ ëª¨ë“œ: ë©€í‹°ìŠ¤ë ˆë”© íŒŒì´í”„ë¼ì¸ + ì§„ì •í•œ ë°°ì¹˜ ì¶”ë¡ 
        - ë¹„ë™ê¸° ë””ì½”ë”© (ìŠ¤ë ˆë“œ)
        - ì§„ì •í•œ ë°°ì¹˜ YOLO ì¶”ë¡  (GPU)
        - ë¹„ë™ê¸° NVENC ì¸ì½”ë”© (ìŠ¤ë ˆë“œ)
        
        RTX 4070 SUPER ê¸°ì¤€ 4K 60fpsì—ì„œ ~43 fps ë‹¬ì„±
        """
        import subprocess
        from threading import Thread
        from queue import Queue, Empty
        
        # ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        cap = cv2.VideoCapture(input_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        start_frame = int(start_time * fps) if start_time else 0
        end_frame = int(end_time * fps) if end_time else total_frames
        process_frames = end_frame - start_frame
        
        if start_frame > 0:
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
        
        # ìµœì í™” ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        encode_settings = self.optimal_settings if self.optimal_settings else {
            'queue_size': self.queue_size,
            'ffmpeg_threads': 16,
            'nvenc_preset': 'p4',
            'nvenc_lookahead': 32,
            'nvenc_surfaces': 16,
            'nvenc_bframes': 4,
            'use_spatial_aq': True,
            'use_temporal_aq': True,
        }

        logger.info(f"ê³ ì„±ëŠ¥ ëª¨ë“œ (ë©€í‹°ìŠ¤ë ˆë”© íŒŒì´í”„ë¼ì¸ + ë°°ì¹˜ GPU ì¶”ë¡ )")
        logger.info(f"   [ì¶”ë¡ ] ë°°ì¹˜: {self.batch_size}, ê°ì§€ê°„ê²©: {self.detect_interval}, FP16: {self.use_fp16}")
        logger.info(f"   [ì¸ì½”ë”©] NVENC {encode_settings.get('nvenc_preset', 'p4')}, "
                   f"Lookahead: {encode_settings.get('nvenc_lookahead', 32)}, "
                   f"Surfaces: {encode_settings.get('nvenc_surfaces', 16)}")
        logger.info(f"   [ì‹œìŠ¤í…œ] í: {self.queue_size}, ìŠ¤ë ˆë“œ: {encode_settings.get('ffmpeg_threads', 16)}")
        logger.info(f"   ì²˜ë¦¬ í”„ë ˆì„: {process_frames:,}")

        # í ì„¤ì • (ëŒ€ìš©ëŸ‰ RAM í™œìš©)
        decode_queue = Queue(maxsize=self.queue_size)
        encode_queue = Queue(maxsize=self.queue_size)
        done_decode = [False]
        done_process = [False]

        # ìµœì í™”ëœ NVENC ì¸ì½”ë” ì‹œì‘
        encode_cmd = build_nvenc_command(
            output_path, width, height, fps,
            encode_settings, use_hevc=use_hevc
        )
        logger.info(f"   ì¸ì½”ë”: {' '.join(encode_cmd[:8])}...")

        # í° ë²„í¼ë¡œ ì¸ì½”ë” ì‹œì‘ (4K í”„ë ˆì„ = ~24MB)
        frame_buffer_size = width * height * 3 * 32  # 32í”„ë ˆì„ ë²„í¼
        encoder = subprocess.Popen(
            encode_cmd,
            stdin=subprocess.PIPE,
            bufsize=frame_buffer_size
        )
        
        # í†µê³„
        stats = {'processed': 0, 'persons': 0, 'vehicles': 0}
        start_total_time = time.time()
        
        def decoder_thread():
            """ë¹„ë™ê¸° í”„ë ˆì„ ë””ì½”ë”©"""
            count = 0
            while count < process_frames:
                ret, frame = cap.read()
                if not ret:
                    break
                decode_queue.put((count, frame))
                count += 1
            done_decode[0] = True
        
        def processor_thread():
            """ë°°ì¹˜ GPU ì¶”ë¡  + ë§ˆìŠ¤í‚¹"""
            batch_frames = []
            batch_indices = []
            
            while True:
                try:
                    item = decode_queue.get(timeout=0.3)
                except Empty:
                    if done_decode[0] and decode_queue.empty():
                        break
                    continue
                
                idx, frame = item
                batch_frames.append(frame)
                batch_indices.append(idx)
                
                if len(batch_frames) >= self.batch_size:
                    # ë°°ì¹˜ ì²˜ë¦¬
                    results, n_p, n_v = self.process_batch_parallel(batch_frames, batch_indices)
                    stats['persons'] += n_p
                    stats['vehicles'] += n_v
                    
                    for i, result_frame in enumerate(results):
                        encode_queue.put((batch_indices[i], result_frame))
                    
                    stats['processed'] += len(batch_frames)
                    batch_frames = []
                    batch_indices = []
            
            # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
            if batch_frames:
                results, n_p, n_v = self.process_batch_parallel(batch_frames, batch_indices)
                stats['persons'] += n_p
                stats['vehicles'] += n_v
                for i, result_frame in enumerate(results):
                    encode_queue.put((batch_indices[i], result_frame))
                stats['processed'] += len(batch_frames)
            
            done_process[0] = True
        
        def encoder_thread():
            """ë¹„ë™ê¸° í”„ë ˆì„ ì¸ì½”ë”© (ìˆœì„œ ë³´ì¥)"""
            pending = {}
            next_idx = 0
            
            while True:
                try:
                    item = encode_queue.get(timeout=0.3)
                except Empty:
                    if done_process[0] and encode_queue.empty():
                        break
                    continue
                
                idx, frame = item
                pending[idx] = frame
                
                # ìˆœì„œëŒ€ë¡œ ì“°ê¸°
                while next_idx in pending:
                    encoder.stdin.write(pending.pop(next_idx).tobytes())
                    next_idx += 1
            
            # ë‚¨ì€ í”„ë ˆì„ ì“°ê¸°
            while next_idx in pending:
                encoder.stdin.write(pending.pop(next_idx).tobytes())
                next_idx += 1
        
        # ìŠ¤ë ˆë“œ ì‹œì‘
        threads = [
            Thread(target=decoder_thread, name='Decoder'),
            Thread(target=processor_thread, name='Processor'),
            Thread(target=encoder_thread, name='Encoder')
        ]
        
        for t in threads:
            t.start()
        
        # ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
        last_log = 0
        while any(t.is_alive() for t in threads):
            time.sleep(1)
            if stats['processed'] - last_log >= 100:
                elapsed = time.time() - start_total_time
                avg_fps = stats['processed'] / elapsed if elapsed > 0 else 0
                progress = stats['processed'] / process_frames * 100
                logger.info(f"[{progress:5.1f}%] {stats['processed']:,}/{process_frames:,} | "
                           f"{avg_fps:.1f} fps | ì‚¬ëŒ: {stats['persons']}, ì°¨ëŸ‰: {stats['vehicles']}")
                last_log = stats['processed']
        
        # ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        for t in threads:
            t.join()
        
        # ì¸ì½”ë” ì¢…ë£Œ
        encoder.stdin.close()
        encoder.wait()
        cap.release()
        
        total_time = time.time() - start_total_time
        avg_fps = stats['processed'] / total_time if total_time > 0 else 0
        
        logger.info("-" * 60)
        logger.info(f"âœ… ì™„ë£Œ! í”„ë ˆì„: {stats['processed']:,}, ì‹œê°„: {total_time/60:.1f}ë¶„, FPS: {avg_fps:.1f}")
        logger.info(f"   ì´ ë§ˆìŠ¤í‚¹: ì‚¬ëŒ {stats['persons']:,}, ì°¨ëŸ‰ {stats['vehicles']:,}")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if self.device == 'cuda':
            import torch
            torch.cuda.empty_cache()
        
        return output_path

    def process_video(
        self,
        input_path: str,
        output_path: str = None,
        start_time: float = None,
        end_time: float = None,
        use_hevc: bool = False,
        log_file: str = None,
        verbose: bool = False,
        **kwargs
    ):
        """ë¹„ë””ì˜¤ ì „ì²´ ì²˜ë¦¬ (ë©€í‹°ìŠ¤ë ˆë”©)"""
        if log_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            log_file = str(Path(input_path).parent / f"masking_{timestamp}.log")

        logger = setup_logger(log_file, verbose)
        logger.info("=" * 60)
        logger.info("ìµœì í™” ë§ˆìŠ¤í‚¹ v3.0 ì‹œì‘")
        logger.info("=" * 60)

        # ì¶œë ¥ ê²½ë¡œ ê²°ì •
        if output_path is None:
            input_stem = Path(input_path).stem
            suffix = f"_{int(start_time//60)}m-{int(end_time//60)}m" if start_time else ""
            output_path = str(Path(input_path).parent / f"{input_stem}{suffix}_masked.mp4")

        # ê³ ì„±ëŠ¥ ëª¨ë“œì¼ ë•Œ FFmpeg íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
        if self.high_performance:
            return self.process_video_high_performance(
                input_path, output_path, start_time, end_time, use_hevc, logger
            )

        start_total_time = time.time()

        cap = cv2.VideoCapture(input_path)
        if not cap.isOpened():
            raise ValueError(f"ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")

        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        start_frame = int(start_time * fps) if start_time else 0
        end_frame = int(end_time * fps) if end_time else total_frames
        end_frame = min(end_frame, total_frames)
        process_frames = end_frame - start_frame

        logger.info(f"ì…ë ¥: {input_path}")
        logger.info(f"í•´ìƒë„: {width}x{height}, FPS: {fps:.2f}")
        logger.info(f"ë””ë°”ì´ìŠ¤: {self.device}, ë°°ì¹˜: {self.batch_size}, ê°ì§€ê°„ê²©: {self.detect_interval}")

        if start_frame > 0:
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)

        if use_hevc:
            temp_output = tempfile.NamedTemporaryFile(suffix='.mp4', delete=False).name
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(temp_output, fourcc, fps, (width, height))
        else:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        logger.info(f"ì¶œë ¥: {output_path}")
        logger.info("-" * 60)

        # ë©€í‹°ìŠ¤ë ˆë”©
        read_queue = Queue(maxsize=self.queue_size)
        write_queue = Queue(maxsize=self.queue_size)

        reader = FrameReader(cap, read_queue, start_frame, end_frame)
        writer = FrameWriter(out, write_queue)

        reader.start()
        writer.start()

        processed_count = 0
        total_persons = 0
        total_vehicles = 0
        errors = []
        frame_times = []
        batch_frames = []
        batch_indices = []

        try:
            while True:
                try:
                    item = read_queue.get(timeout=1.0)
                except Empty:
                    if not reader.is_alive():
                        break
                    continue

                if item is None:
                    if batch_frames:
                        batch_start = time.time()
                        for frame, idx in zip(batch_frames, batch_indices):
                            frame, n_p, n_v = self.process_frame_optimized(frame, idx)
                            total_persons += n_p
                            total_vehicles += n_v
                            write_queue.put((idx, frame))
                        frame_times.append((time.time() - batch_start) / len(batch_frames))
                    break

                frame_idx, frame = item
                batch_frames.append(frame)
                batch_indices.append(processed_count)
                processed_count += 1

                if len(batch_frames) >= self.batch_size:
                    batch_start = time.time()
                    for frame, idx in zip(batch_frames, batch_indices):
                        try:
                            frame, n_p, n_v = self.process_frame_optimized(frame, idx)
                            total_persons += n_p
                            total_vehicles += n_v
                            write_queue.put((idx, frame))
                        except Exception as e:
                            errors.append(str(e))
                            write_queue.put((idx, frame))
                    frame_times.append((time.time() - batch_start) / len(batch_frames))
                    batch_frames = []
                    batch_indices = []

                if processed_count % 60 == 0:
                    progress = processed_count / process_frames * 100
                    elapsed = time.time() - start_total_time
                    avg_fps = processed_count / elapsed if elapsed > 0 else 0
                    logger.info(f"[{progress:5.1f}%] {processed_count:,}/{process_frames:,} | "
                               f"{avg_fps:.1f} fps | ì‚¬ëŒ: {total_persons}, ì°¨ëŸ‰: {total_vehicles}")

        except KeyboardInterrupt:
            logger.warning("ì‚¬ìš©ì ì¤‘ë‹¨")

        finally:
            write_queue.put(None)
            reader.stop()
            # writerê°€ ì™„ì „íˆ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
            if not writer.wait_finished(timeout=60):
                logger.warning("Writer íƒ€ì„ì•„ì›ƒ, ê°•ì œ ì¢…ë£Œ")
                writer.stop()
            writer.join(timeout=5)
            # VideoCapture/Writer ì•ˆì „í•˜ê²Œ í•´ì œ
            try:
                cap.release()
            except Exception:
                pass
            try:
                out.release()
            except Exception:
                pass

        masking_time = time.time() - start_total_time
        logger.info("-" * 60)
        logger.info(f"ë§ˆìŠ¤í‚¹ ì™„ë£Œ! í”„ë ˆì„: {processed_count}, ì‹œê°„: {masking_time/60:.1f}ë¶„")

        # HEVC ì¸ì½”ë”© (ìµœì í™”ëœ ì„¤ì •)
        if use_hevc:
            logger.info("HEVC ì¸ì½”ë”© (NVENC ìµœì í™”)...")
            use_nvenc = self.device == 'cuda'

            if use_nvenc:
                # ìµœì í™” ì„¤ì • ì ìš©
                settings = self.optimal_settings if self.optimal_settings else {
                    'nvenc_preset': 'p4',
                    'nvenc_lookahead': 32,
                    'nvenc_surfaces': 16,
                    'nvenc_bframes': 4,
                    'use_spatial_aq': True,
                    'use_temporal_aq': True,
                    'ffmpeg_threads': 16,
                }

                ffmpeg_cmd = [
                    'ffmpeg', '-y', '-hide_banner', '-loglevel', 'warning',
                    '-i', temp_output,
                    '-c:v', 'hevc_nvenc',
                    '-preset', settings.get('nvenc_preset', 'p4'),
                    '-tune', 'hq',
                    '-rc', 'vbr', '-cq', '23', '-b:v', '0',
                    '-rc-lookahead', str(settings.get('nvenc_lookahead', 32)),
                    '-surfaces', str(settings.get('nvenc_surfaces', 16)),
                    '-spatial-aq', '1', '-aq-strength', '8',
                    '-temporal-aq', '1',
                    '-bf', str(settings.get('nvenc_bframes', 4)),
                    '-b_ref_mode', 'middle',
                    '-tag:v', 'hvc1',
                    '-an', output_path
                ]
            else:
                ffmpeg_cmd = [
                    'ffmpeg', '-y', '-i', temp_output,
                    '-c:v', 'libx265', '-preset', 'medium', '-crf', '23',
                    '-tag:v', 'hvc1', '-an', output_path
                ]

            try:
                result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)
                if result.returncode != 0 and use_nvenc:
                    logger.warning("NVENC ì‹¤íŒ¨, libx265ë¡œ ì¬ì‹œë„...")
                    ffmpeg_cmd = [
                        'ffmpeg', '-y', '-i', temp_output,
                        '-c:v', 'libx265', '-preset', 'medium', '-crf', '23',
                        '-tag:v', 'hvc1', '-an', output_path
                    ]
                    result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)

                if result.returncode == 0:
                    logger.info("HEVC ì¸ì½”ë”© ì™„ë£Œ!")
                    os.unlink(temp_output)
                else:
                    logger.error(f"HEVC ì¸ì½”ë”© ì‹¤íŒ¨: {result.stderr}")
            except Exception as e:
                logger.error(f"ffmpeg ì˜¤ë¥˜: {e}")

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if self.device == 'cuda':
            try:
                import torch
                torch.cuda.empty_cache()
            except Exception:
                pass

        total_time = time.time() - start_total_time
        logger.info("=" * 60)
        logger.info(f"ì™„ë£Œ! ì¶œë ¥: {output_path}")
        logger.info(f"ì´ ì‹œê°„: {total_time/60:.1f}ë¶„")
        logger.info("=" * 60)

        return output_path

    # ========================================================
    # 2-Pass ëª¨ë“œ: ë¶„ì„ê³¼ ì¸ì½”ë”© ë¶„ë¦¬
    # ========================================================

    def analyze_video(
        self,
        input_path: str,
        output_json: str = None,
        start_time: float = None,
        end_time: float = None,
        log_file: str = None,
        verbose: bool = False,
    ):
        """
        Pass 1: ë¹„ë””ì˜¤ ë¶„ì„ë§Œ ìˆ˜í–‰í•˜ê³  ë§ˆìŠ¤í‚¹ ì¢Œí‘œë¥¼ JSONìœ¼ë¡œ ì €ì¥ (ë©€í‹°ìŠ¤ë ˆë”©)

        GPUë¥¼ YOLO ì¶”ë¡ ì—ë§Œ 100% í™œìš©
        2-ìŠ¤ë ˆë“œ íŒŒì´í”„ë¼ì¸: ë””ì½”ë” â†’ ë¶„ì„ê¸°

        Returns:
            str: ìƒì„±ëœ JSON íŒŒì¼ ê²½ë¡œ
        """
        from threading import Thread
        from queue import Queue, Empty

        if log_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            log_file = str(Path(input_path).parent / f"analyze_{timestamp}.log")

        logger = setup_logger(log_file, verbose)
        logger.info("=" * 60)
        logger.info("2-Pass ëª¨ë“œ: Pass 1 (ë¶„ì„) - ë©€í‹°ìŠ¤ë ˆë”©")
        logger.info("GPU 100% YOLO ì¶”ë¡  ì „ìš©")
        logger.info("=" * 60)

        start_total_time = time.time()

        # ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (cv2ë¡œ)
        cap = cv2.VideoCapture(input_path)
        if not cap.isOpened():
            raise ValueError(f"ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")

        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        start_frame = int(start_time * fps) if start_time else 0
        end_frame = int(end_time * fps) if end_time else total_frames
        end_frame = min(end_frame, total_frames)
        process_frames = end_frame - start_frame

        queue_size = self.queue_size

        # NVDEC ì‚¬ìš© ì—¬ë¶€ ê²°ì •
        use_nvdec = self.use_nvdec

        logger.info(f"ì…ë ¥: {input_path}")
        logger.info(f"í•´ìƒë„: {width}x{height}, FPS: {fps:.2f}")
        logger.info(f"ì²˜ë¦¬ í”„ë ˆì„: {process_frames:,}")
        logger.info(f"ë””ë°”ì´ìŠ¤: {self.device}, ë°°ì¹˜: {self.batch_size}, FP16: {self.use_fp16}")
        logger.info(f"ë””ì½”ë”©: {'NVDEC (GPU)' if use_nvdec else 'CPU'}")
        logger.info(f"ë©€í‹°ìŠ¤ë ˆë”©: í={queue_size}")

        # NVDEC ì‚¬ìš© ì‹œ cv2 ë‹«ê³  NVDEC ë””ì½”ë” ì¤€ë¹„
        if use_nvdec:
            cap.release()
            nvdec_decoder = NVDECDecoder(
                input_path, width, height,
                start_time=start_time, end_time=end_time
            )
        else:
            nvdec_decoder = None
            if start_frame > 0:
                cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)

        # JSON ì¶œë ¥ ê²½ë¡œ
        if output_json is None:
            input_stem = Path(input_path).stem
            suffix = f"_{int(start_time//60)}m-{int(end_time//60)}m" if start_time else ""
            output_json = str(Path(input_path).parent / f"{input_stem}{suffix}_masks.json")

        # ë§ˆìŠ¤í¬ ë°ì´í„° êµ¬ì¡°
        mask_data = {
            'version': '2.0',
            'source': str(input_path),
            'video_info': {
                'width': width,
                'height': height,
                'fps': fps,
                'total_frames': total_frames,
                'start_frame': start_frame,
                'end_frame': end_frame,
            },
            'settings': {
                'mask_persons': self.mask_persons,
                'mask_plates': self.mask_plates,
                'person_expand': self.person_expand,
                'plate_expand': self.plate_expand,
                'plate_detect_mode': self.plate_detect_mode,
            },
            'frames': {}  # frame_idx -> {'persons': [...], 'plates': [...]}
        }

        logger.info("-" * 60)

        # í ë° ë™ê¸°í™” ì„¤ì •
        decode_queue = Queue(maxsize=queue_size)
        done_decode = [False]

        # í†µê³„
        stats = {'processed': 0, 'persons': 0, 'plates': 0}

        # ì°¸ì¡° ìº¡ì²˜
        mask_persons = self.mask_persons
        mask_plates = self.mask_plates
        person_expand = self.person_expand
        plate_expand = self.plate_expand
        plate_detect_mode = self.plate_detect_mode
        max_mask_ratio = self.max_mask_ratio
        detect_interval = self.detect_interval

        def decoder_thread():
            """ë¹„ë™ê¸° í”„ë ˆì„ ë””ì½”ë”© (NVDEC ë˜ëŠ” CPU)"""
            nonlocal nvdec_decoder
            count = 0

            if use_nvdec:
                # NVDEC GPU ë””ì½”ë”©
                nvdec_decoder.start()
                while count < process_frames:
                    frame = nvdec_decoder.read_frame()
                    if frame is None:
                        break
                    decode_queue.put((count, frame))
                    count += 1
                nvdec_decoder.close()
            else:
                # CPU ë””ì½”ë”© (cv2)
                while count < process_frames:
                    ret, frame = cap.read()
                    if not ret:
                        break
                    decode_queue.put((count, frame))
                    count += 1

            done_decode[0] = True

        def analyzer_thread():
            """YOLO ë¶„ì„ ë° ë§ˆìŠ¤í¬ ì¢Œí‘œ ì¶”ì¶œ"""
            while True:
                try:
                    item = decode_queue.get(timeout=0.5)
                except Empty:
                    if done_decode[0] and decode_queue.empty():
                        break
                    continue

                frame_idx, frame = item

                # YOLO ê°ì§€ (ê°„ê²©ì— ë”°ë¼)
                if frame_idx % detect_interval == 0:
                    detections = self.detect_all(frame, frame_idx)
                    self.person_interpolator.update(frame_idx, detections['persons'])
                    self.vehicle_interpolator.update(frame_idx, detections['vehicles'])

                # ë³´ê°„ëœ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                persons = self.person_interpolator.get_interpolated(frame_idx)
                vehicles = self.vehicle_interpolator.get_interpolated(frame_idx)

                frame_masks = {'persons': [], 'plates': []}

                # ì‚¬ëŒ ë§ˆìŠ¤í‚¹ ì¢Œí‘œ
                if mask_persons:
                    for person in persons:
                        box = expand_box(person['box'], person_expand, frame.shape)
                        validated = validate_and_clip_box(box, frame.shape, max_mask_ratio)
                        if validated:
                            frame_masks['persons'].append(list(validated))
                            stats['persons'] += 1

                # ë²ˆí˜¸íŒ ë§ˆìŠ¤í‚¹ ì¢Œí‘œ
                if mask_plates:
                    for vehicle in vehicles:
                        track_id = vehicle.get('track_id')
                        vehicle_box = vehicle['box']

                        if plate_detect_mode == "legacy":
                            plate_box = get_plate_region(vehicle_box, plate_expand)
                            validated = validate_and_clip_box(plate_box, frame.shape, 0.15)
                            if validated:
                                frame_masks['plates'].append(list(validated))
                                stats['plates'] += 1
                        else:
                            use_detection = (plate_detect_mode == "auto")

                            if frame_idx % detect_interval == 0:
                                detected_plates = get_all_plate_regions(
                                    frame, vehicle_box,
                                    expand_ratio=plate_expand,
                                    use_detection=use_detection
                                )
                                self.plate_tracker.update(frame_idx, track_id, detected_plates)

                            plate_regions = self.plate_tracker.get_plates(frame_idx, track_id, vehicle_box)
                            if not plate_regions:
                                plate_regions = get_all_plate_regions(
                                    frame, vehicle_box,
                                    expand_ratio=plate_expand,
                                    use_detection=use_detection
                                )

                            for plate_box in plate_regions:
                                validated = validate_and_clip_box(plate_box, frame.shape, 0.15)
                                if validated:
                                    frame_masks['plates'].append(list(validated))
                                    stats['plates'] += 1

                # ë§ˆìŠ¤í¬ê°€ ìˆëŠ” í”„ë ˆì„ë§Œ ì €ì¥
                if frame_masks['persons'] or frame_masks['plates']:
                    mask_data['frames'][str(frame_idx)] = frame_masks

                stats['processed'] += 1

                # ìºì‹œ ì •ë¦¬
                if frame_idx % 30 == 0:
                    self.plate_tracker.cleanup(frame_idx)

        # ìŠ¤ë ˆë“œ ì‹œì‘
        decoder = Thread(target=decoder_thread, name='Decoder')
        analyzer = Thread(target=analyzer_thread, name='Analyzer')

        decoder.start()
        analyzer.start()

        # ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
        last_log = 0
        while decoder.is_alive() or analyzer.is_alive():
            time.sleep(1)
            if stats['processed'] - last_log >= 200:
                elapsed = time.time() - start_total_time
                avg_fps = stats['processed'] / elapsed if elapsed > 0 else 0
                progress = stats['processed'] / process_frames * 100
                logger.info(f"[{progress:5.1f}%] {stats['processed']:,}/{process_frames:,} | "
                           f"{avg_fps:.1f} fps | ì‚¬ëŒ: {stats['persons']:,}, ë²ˆí˜¸íŒ: {stats['plates']:,}")
                last_log = stats['processed']

        # ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        decoder.join()
        analyzer.join()

        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (NVDEC ë¯¸ì‚¬ìš© ì‹œì—ë§Œ)
        if not use_nvdec:
            cap.release()

        # JSON ì €ì¥
        mask_data['stats'] = {
            'frames_with_masks': len(mask_data['frames']),
            'total_persons': stats['persons'],
            'total_plates': stats['plates'],
        }

        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(mask_data, f, ensure_ascii=False)

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if self.device == 'cuda':
            try:
                import torch
                torch.cuda.empty_cache()
            except Exception:
                pass

        total_time = time.time() - start_total_time
        avg_fps = stats['processed'] / total_time if total_time > 0 else 0

        logger.info("=" * 60)
        logger.info(f"Pass 1 ì™„ë£Œ!")
        logger.info(f"   ë¶„ì„ í”„ë ˆì„: {stats['processed']:,}")
        logger.info(f"   ë§ˆìŠ¤í¬ í”„ë ˆì„: {len(mask_data['frames']):,}")
        logger.info(f"   ì´ ë§ˆìŠ¤í‚¹: ì‚¬ëŒ {stats['persons']:,}, ë²ˆí˜¸íŒ {stats['plates']:,}")
        logger.info(f"   ì²˜ë¦¬ ì†ë„: {avg_fps:.1f} fps")
        logger.info(f"   ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
        logger.info(f"   JSON ì €ì¥: {output_json}")
        logger.info("=" * 60)

        return output_json

    def encode_with_masks(
        self,
        input_path: str,
        mask_json: str,
        output_path: str = None,
        use_hevc: bool = False,
        log_file: str = None,
        verbose: bool = False,
    ):
        """
        Pass 2: JSON ë§ˆìŠ¤í¬ ë°ì´í„°ë¥¼ ì ìš©í•˜ê³  ì¸ì½”ë”© (ë©€í‹°ìŠ¤ë ˆë”©)

        GPUë¥¼ NVENC ì¸ì½”ë”©ì—ë§Œ 100% í™œìš©
        3-ìŠ¤ë ˆë“œ íŒŒì´í”„ë¼ì¸: ë””ì½”ë” â†’ ë§ˆìŠ¤ì»¤ â†’ ì¸ì½”ë”

        Returns:
            str: ìƒì„±ëœ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        """
        from threading import Thread
        from queue import Queue, Empty

        if log_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            log_file = str(Path(input_path).parent / f"encode_{timestamp}.log")

        logger = setup_logger(log_file, verbose)
        logger.info("=" * 60)
        logger.info("2-Pass ëª¨ë“œ: Pass 2 (ì¸ì½”ë”©) - ë©€í‹°ìŠ¤ë ˆë”©")
        logger.info("GPU: NVDEC ë””ì½”ë”© + NVENC ì¸ì½”ë”©")
        logger.info("=" * 60)

        # JSON ë§ˆìŠ¤í¬ ë°ì´í„° ë¡œë“œ
        with open(mask_json, 'r', encoding='utf-8') as f:
            mask_data = json.load(f)

        video_info = mask_data['video_info']
        frames_data = mask_data['frames']
        stats = mask_data.get('stats', {})

        logger.info(f"ë§ˆìŠ¤í¬ ë°ì´í„°: {mask_json}")
        logger.info(f"   ë§ˆìŠ¤í¬ í”„ë ˆì„: {len(frames_data):,}")
        logger.info(f"   ì´ ì‚¬ëŒ: {stats.get('total_persons', 0):,}")
        logger.info(f"   ì´ ë²ˆí˜¸íŒ: {stats.get('total_plates', 0):,}")

        start_total_time = time.time()

        # ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        cap = cv2.VideoCapture(input_path)
        if not cap.isOpened():
            raise ValueError(f"ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")

        fps = video_info['fps']
        width = video_info['width']
        height = video_info['height']
        start_frame = video_info['start_frame']
        end_frame = video_info['end_frame']
        process_frames = end_frame - start_frame

        # NVDEC ì‚¬ìš© ì—¬ë¶€
        use_nvdec = self.use_nvdec
        start_time_sec = start_frame / fps if start_frame > 0 else None
        end_time_sec = end_frame / fps if end_frame < video_info.get('total_frames', end_frame) else None

        logger.info(f"ì…ë ¥: {input_path}")
        logger.info(f"í•´ìƒë„: {width}x{height}, FPS: {fps:.2f}")
        logger.info(f"ì²˜ë¦¬ í”„ë ˆì„: {process_frames:,}")
        logger.info(f"ë””ì½”ë”©: {'NVDEC (GPU)' if use_nvdec else 'CPU'}")

        if output_path is None:
            input_stem = Path(input_path).stem
            output_path = str(Path(input_path).parent / f"{input_stem}_masked.mp4")

        # NVDEC ì‚¬ìš© ì‹œ cv2 ë‹«ê³  NVDEC ë””ì½”ë” ì¤€ë¹„
        if use_nvdec:
            cap.release()
            nvdec_decoder = NVDECDecoder(
                input_path, width, height,
                start_time=start_time_sec, end_time=end_time_sec
            )
        else:
            nvdec_decoder = None
            if start_frame > 0:
                cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)

        # ìµœì í™”ëœ NVENC ì¸ì½”ë” ì„¤ì •
        encode_settings = self.optimal_settings if self.optimal_settings else {
            'queue_size': 512,
            'ffmpeg_threads': 16,
            'nvenc_preset': 'p4',
            'nvenc_lookahead': 32,
            'nvenc_surfaces': 16,
            'nvenc_bframes': 4,
            'use_spatial_aq': True,
            'use_temporal_aq': True,
        }

        queue_size = encode_settings.get('queue_size', 512)

        encode_cmd = build_nvenc_command(
            output_path, width, height, fps,
            encode_settings, use_hevc=use_hevc
        )

        logger.info(f"ì¶œë ¥: {output_path}")
        logger.info(f"ì½”ë±: {'HEVC' if use_hevc else 'H.264'} (NVENC)")
        logger.info(f"NVENC: preset={encode_settings.get('nvenc_preset')}, "
                   f"lookahead={encode_settings.get('nvenc_lookahead')}, "
                   f"surfaces={encode_settings.get('nvenc_surfaces')}")
        logger.info(f"ë©€í‹°ìŠ¤ë ˆë”©: í={queue_size}")
        logger.info("-" * 60)

        # í ì„¤ì •
        decode_queue = Queue(maxsize=queue_size)
        encode_queue = Queue(maxsize=queue_size)
        done_decode = [False]
        done_mask = [False]

        # í†µê³„
        stats_counter = {'processed': 0, 'masks_applied': 0}

        # í° ë²„í¼ë¡œ ì¸ì½”ë” ì‹œì‘
        frame_buffer_size = width * height * 3 * 64
        encoder = subprocess.Popen(
            encode_cmd,
            stdin=subprocess.PIPE,
            bufsize=frame_buffer_size
        )

        # ë§ˆìŠ¤í‚¹ ì„¤ì • ìº¡ì²˜
        mask_type = self.mask_type
        blur_strength = self.blur_strength
        mosaic_size = self.mosaic_size

        def decoder_thread():
            """ë¹„ë™ê¸° í”„ë ˆì„ ë””ì½”ë”© (NVDEC ë˜ëŠ” CPU)"""
            nonlocal nvdec_decoder
            count = 0

            if use_nvdec:
                # NVDEC GPU ë””ì½”ë”©
                nvdec_decoder.start()
                while count < process_frames:
                    frame = nvdec_decoder.read_frame()
                    if frame is None:
                        break
                    decode_queue.put((count, frame))
                    count += 1
                nvdec_decoder.close()
            else:
                # CPU ë””ì½”ë”© (cv2)
                while count < process_frames:
                    ret, frame = cap.read()
                    if not ret:
                        break
                    decode_queue.put((count, frame))
                    count += 1

            done_decode[0] = True

        def masker_thread():
            """ë¹„ë™ê¸° ë§ˆìŠ¤í¬ ì ìš©"""
            while True:
                try:
                    item = decode_queue.get(timeout=0.5)
                except Empty:
                    if done_decode[0] and decode_queue.empty():
                        break
                    continue

                frame_idx, frame = item
                frame_key = str(frame_idx)

                if frame_key in frames_data:
                    frame_masks = frames_data[frame_key]

                    # ì‚¬ëŒ ë§ˆìŠ¤í‚¹
                    for box in frame_masks.get('persons', []):
                        x1, y1, x2, y2 = box
                        if mask_type == "blur":
                            frame = apply_blur(frame, x1, y1, x2, y2, blur_strength)
                        else:
                            frame = apply_mosaic(frame, x1, y1, x2, y2, mosaic_size)
                        stats_counter['masks_applied'] += 1

                    # ë²ˆí˜¸íŒ ë§ˆìŠ¤í‚¹
                    for box in frame_masks.get('plates', []):
                        x1, y1, x2, y2 = box
                        if mask_type == "blur":
                            frame = apply_blur(frame, x1, y1, x2, y2, blur_strength)
                        else:
                            frame = apply_mosaic(frame, x1, y1, x2, y2, mosaic_size)
                        stats_counter['masks_applied'] += 1

                encode_queue.put((frame_idx, frame))
                stats_counter['processed'] += 1

            done_mask[0] = True

        def encoder_thread():
            """ë¹„ë™ê¸° NVENC ì¸ì½”ë”© (ìˆœì„œ ë³´ì¥)"""
            pending = {}
            next_idx = 0

            while True:
                try:
                    item = encode_queue.get(timeout=0.5)
                except Empty:
                    if done_mask[0] and encode_queue.empty():
                        break
                    continue

                frame_idx, frame = item
                pending[frame_idx] = frame

                # ìˆœì„œëŒ€ë¡œ ì¸ì½”ë”©
                while next_idx in pending:
                    encoder.stdin.write(pending.pop(next_idx).tobytes())
                    next_idx += 1

            # ë‚¨ì€ í”„ë ˆì„ ì²˜ë¦¬
            while next_idx in pending:
                encoder.stdin.write(pending.pop(next_idx).tobytes())
                next_idx += 1

        # ìŠ¤ë ˆë“œ ì‹œì‘
        threads = [
            Thread(target=decoder_thread, name='Decoder'),
            Thread(target=masker_thread, name='Masker'),
            Thread(target=encoder_thread, name='Encoder')
        ]

        for t in threads:
            t.start()

        # ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
        last_log = 0
        while any(t.is_alive() for t in threads):
            time.sleep(1)
            if stats_counter['processed'] - last_log >= 200:
                elapsed = time.time() - start_total_time
                avg_fps = stats_counter['processed'] / elapsed if elapsed > 0 else 0
                progress = stats_counter['processed'] / process_frames * 100
                logger.info(f"[{progress:5.1f}%] {stats_counter['processed']:,}/{process_frames:,} | "
                           f"{avg_fps:.1f} fps | ë§ˆìŠ¤í¬: {stats_counter['masks_applied']:,}")
                last_log = stats_counter['processed']

        # ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        for t in threads:
            t.join()

        # ì¸ì½”ë” ì¢…ë£Œ
        encoder.stdin.close()
        encoder.wait()

        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (NVDEC ë¯¸ì‚¬ìš© ì‹œì—ë§Œ)
        if not use_nvdec:
            cap.release()

        total_time = time.time() - start_total_time
        avg_fps = stats_counter['processed'] / total_time if total_time > 0 else 0

        logger.info("=" * 60)
        logger.info(f"Pass 2 ì™„ë£Œ!")
        logger.info(f"   ì¸ì½”ë”© í”„ë ˆì„: {stats_counter['processed']:,}")
        logger.info(f"   ì ìš©ëœ ë§ˆìŠ¤í¬: {stats_counter['masks_applied']:,}")
        logger.info(f"   ì²˜ë¦¬ ì†ë„: {avg_fps:.1f} fps")
        logger.info(f"   ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
        logger.info(f"   ì¶œë ¥: {output_path}")
        logger.info("=" * 60)

        return output_path

    def process_video_2pass(
        self,
        input_path: str,
        output_path: str = None,
        start_time: float = None,
        end_time: float = None,
        use_hevc: bool = False,
        keep_json: bool = False,
        log_file: str = None,
        verbose: bool = False,
    ):
        """
        2-Pass ëª¨ë“œ ì „ì²´ ì‹¤í–‰

        Pass 1: ë¶„ì„ (GPU â†’ YOLO 100%)
        Pass 2: ì¸ì½”ë”© (GPU â†’ NVENC 100%)

        ê° Passì—ì„œ GPUë¥¼ ìµœëŒ€í•œ í™œìš©
        """
        if log_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            log_file = str(Path(input_path).parent / f"2pass_{timestamp}.log")

        logger = setup_logger(log_file, verbose)
        logger.info("=" * 60)
        logger.info("2-Pass ëª¨ë“œ ì‹œì‘")
        logger.info("=" * 60)

        start_total = time.time()

        # Pass 1: ë¶„ì„
        logger.info("\n>>> Pass 1: ë¶„ì„ ì‹œì‘")
        json_path = self.analyze_video(
            input_path,
            start_time=start_time,
            end_time=end_time,
            log_file=log_file,
            verbose=verbose
        )

        # Pass 2: ì¸ì½”ë”©
        logger.info("\n>>> Pass 2: ì¸ì½”ë”© ì‹œì‘")
        result_path = self.encode_with_masks(
            input_path,
            json_path,
            output_path=output_path,
            use_hevc=use_hevc,
            log_file=log_file,
            verbose=verbose
        )

        # JSON ì •ë¦¬
        if not keep_json:
            os.unlink(json_path)
            logger.info(f"ì„ì‹œ JSON ì‚­ì œ: {json_path}")

        total_time = time.time() - start_total
        logger.info("=" * 60)
        logger.info(f"2-Pass ì™„ë£Œ! ì´ ì‹œê°„: {total_time/60:.1f}ë¶„")
        logger.info(f"ì¶œë ¥: {result_path}")
        logger.info("=" * 60)

        return result_path

